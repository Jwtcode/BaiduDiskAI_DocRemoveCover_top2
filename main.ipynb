{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 百度网盘AI大赛-文档图片去遮挡比赛第3名方案\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、赛题分析\n",
    "此次大赛主题结合日常生活常见情景展开，人们在使用手机等移动设备扫描证件或者扫描文档、拍摄展示资料的场景中，经常会拍摄到一些手指或者人头等其他因素，对扫描成品的美观和易用性产生了影响。期望同学们通过计算机技术对给定文档图像进行处理，帮助人们去除文档图像中的手指、人头等因素，还原真实的文档资料，提升使用效率。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、 数据分析\n",
    "- 本次比赛最新发布的数据集共包含训练集、A榜测试集、B榜测试集三个部分，其中训练集共14400组样本，A榜测试集共320个样本，B榜测试集共640个样本,抽取一部分数据如图：\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/347f2446875d40d9968da672f483976b7f1e78c7dc594ffab21a45f7c82e792c)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/978ef62d463343f6b4a5e95d81f34cbcc5050a42ee8f460484bdc7de0d5b3937)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1e1986723a4b413d973722ccbea4a4d3606c22f05d8d4e9fad2582a123d6f39e)\n",
    "\n",
    "- hand,head 为带有手指、人头遮挡数据及遮挡部位分割图，gt 为非遮挡图片（仅有训练集数据提供gt ，A榜测试集、B榜测试集数据均不提供gt);\n",
    "- annotation.txt 为图片对应关系，每一行为一组样本，用空格分隔，分别为非遮挡图片、遮挡图片、分割图片;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、评价标准\n",
    "评价指标为 PSNR 和 MSSSIM；\n",
    "\n",
    "用于评价的机器环境仅提供两种框架模型运行环境：paddlepaddle 和 onnxruntime，其他框架模型可转换为\n",
    "上述两种框架的模型；\n",
    "\n",
    "机器配置：V100，显存16G，内存10G；\n",
    "\n",
    "单张图片耗时>2s，决赛中的性能分数记0分。\n",
    "\n",
    "由评价标准可知，不能使用大模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 三、模型设计\n",
    "\n",
    "Baseline的选择：针对图像去模糊这个任务，我们首先查询了paper with code网站，找到了目前优异的几个模型：MADANet、NAFNet以及Restormer。根据paper with code的信息我们可以了解到MADANet采用了额外数据进行训练，并且这个赛道数据量并不是很多。因此我们采用了NAFNet作为我们此次的baseline。\n",
    "网络主体架构为UNet，如图：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c2ced773bf7b4d9db72ed48c4b92999964dd4198103c463282fa46a62cd9d319)\n",
    "\n",
    "其中Encoder和Decoder采用NAFBlock:\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a8b2262aaa144f8ea8c9c69087b0f93d988fc976e70e4e8ca61b2c0e88f274df)\n",
    "\n",
    "\n",
    "由于比赛推理速度需要每张图像1.2s之内，我们对原有的NAFNet进行了通道缩减和减少深度的操作来使得A榜测试图像推理能达到0.8305秒每张图，图中各个进行下采样的block数量设置为1，最深层的block数量设置为10，在单卡V100上训练600000次迭代。为了进一步提升网络性能，我们还进一步采用了Test-time Local Converter (TLC)。主要参考了[Improving Image Restoration by Revisiting Global Information Aggregation论文](https://arxiv.org/abs/2112.04491)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 四、数据增强与清洗\n",
    "\n",
    "### 数据划分\n",
    "官方给的数据为1000张图像，我们将最后的100张图像作为测试集，前面900张作为训练集\n",
    "\n",
    "### 数据增广\n",
    "- 我们对训练集进行了裁剪为1024大小的patch。\n",
    "- 在训练模型的过程中，为了进一步充分利用所有的数据，我们采用了图像翻转，图像旋转等操作来增广数据集。\n",
    "- 为了加快训练过程，我们还采用了随机裁剪策略。\n",
    "\n",
    "### 数据清洗\n",
    "我们发现在裁剪过程中，会裁剪出很多空白图块以及一些信息很少的图块。这些图块并不会帮助网络学习到对应去模糊的知识，因此我们针对这样的情况进行了数据清洗。由于裁剪后的数据有7w+的数据量，筛选非常困难，并且在Aistudio上进行训练的话，需要不断重新导入数据。因此，我们考虑到计算每个图块的平均梯度来度量每个图块包含复杂纹理的多少，并且进行统计。如下所示：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/d7f532be7a444bbfafd4b19c50787c2865b97e7032604963bc511060aaea38b6)\n",
    "\n",
    "\n",
    "从统计图中我们不难看出，大多数图块都分布在20左右，低于10的图块较少。我们在进一步比对裁剪后的数据，发现图像平均梯度小于10的图块大多都是空白以及一些纹理少的图块。因此，我们直接将平均梯度大于10的图块地址写入txt文件中，通过读取txt文件中的地址来读取数据，然后构建训练集。\n",
    "\n",
    "计算梯度主要代码如下：\n",
    "```\n",
    "import cv2\n",
    "import numpy as np\n",
    "def cal_gradient(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    sobelx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=3)\n",
    "    sobelx = cv2.convertScaleAbs(sobelx)\n",
    "    sobely = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=3)\n",
    "    sobely = cv2.convertScaleAbs(sobely)\n",
    "    sobelxy = cv2.addWeighted(sobelx,0.5,sobely,0.5,0)\n",
    "    return np.mean(sobelxy)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 五、训练细节\n",
    "### 训练配置\n",
    "- 总迭代数：600000 iteration\n",
    "- 我们采用了渐进训练的方式，来加速训练过程。分别采用batch size为8和patch size为192来进行训练184000次迭代，batch size为5和patch size为256来进行训练128000次迭代，batch size为4和patch size为320来进行训练96000次迭代，batch size为2和patch size为384来进行训练72000次迭代，batch size为1和patch size为512来进行训练72000次迭代，batch size为1和patch size为1024来进行训练48000次迭代。\n",
    "- 我们采用了余弦退火的学习率策略来优化网络，在184000次迭代以及416000迭代处进行初始学习率\n",
    "- 学习率我们采用2e-4，优化器为AdamW\n",
    "\n",
    "### 训练分为阶段：\n",
    "- 第一阶段损失函数为L1Loss\n",
    "- 第二阶段损失函数使用Charbonnier Loss+MS_SSIMLoss\n",
    "- 最后，在全量数据进行finetuning\n",
    "\n",
    "主要Loss函数如下：\n",
    "```\n",
    "def _ssim(img1, img2, window, window_size, channel=3 ,data_range = 255.,size_average=True,C=None):\n",
    "    # size_average for different channel\n",
    "\n",
    "    padding = window_size // 2\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padding, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padding, groups=channel)\n",
    "    # print(mu1.shape)\n",
    "    # print(mu1[0,0])\n",
    "    # print(mu1.mean())\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padding, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padding, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padding, groups=channel) - mu1_mu2\n",
    "    if C ==None:\n",
    "        C1 = (0.01*data_range) ** 2\n",
    "        C2 = (0.03*data_range) ** 2\n",
    "    else:\n",
    "        C1 = (C[0]*data_range) ** 2\n",
    "        C2 = (C[1]*data_range) ** 2\n",
    "    # l = (2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1)\n",
    "    # ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "    sc = (2 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)\n",
    "    lsc = ((2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1))*sc\n",
    "\n",
    "    if size_average:\n",
    "        ### ssim_map.mean()是对这个tensor里面的所有的数值求平均\n",
    "        return lsc.mean()\n",
    "    else:\n",
    "        # ## 返回各个channel的值\n",
    "        return lsc.flatten(2).mean(-1),sc.flatten(2).mean(-1)\n",
    "\n",
    "def ms_ssim(\n",
    "    img1, img2,window, data_range=255, size_average=True, window_size=11, channel=3, sigma=1.5, weights=None, C=(0.01, 0.03)\n",
    "):\n",
    "\n",
    "    r\"\"\" interface of ms-ssim\n",
    "    Args:\n",
    "        img1 (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
    "        img2 (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
    "        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "        win_size: (int, optional): the size of gauss kernel\n",
    "        win_sigma: (float, optional): sigma of normal distribution\n",
    "        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n",
    "        weights (list, optional): weights for different levels\n",
    "        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
    "    Returns:\n",
    "        torch.Tensor: ms-ssim results\n",
    "    \"\"\"\n",
    "    if not img1.shape == img2.shape:\n",
    "        raise ValueError(\"Input images should have the same dimensions.\")\n",
    "\n",
    "    # for d in range(len(img1.shape) - 1, 1, -1):\n",
    "    #     img1 = img1.squeeze(dim=d)\n",
    "    #     img2 = img2.squeeze(dim=d)\n",
    "\n",
    "    if not img1.dtype == img2.dtype:\n",
    "        raise ValueError(\"Input images should have the same dtype.\")\n",
    "\n",
    "    if len(img1.shape) == 4:\n",
    "        avg_pool = F.avg_pool2d\n",
    "    elif len(img1.shape) == 5:\n",
    "        avg_pool = F.avg_pool3d\n",
    "    else:\n",
    "        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {img1.shape}\")\n",
    "\n",
    "    smaller_side = min(img1.shape[-2:])\n",
    "\n",
    "    assert smaller_side > (window_size - 1) * (2 ** 4), \"Image size should be larger than %d due to the 4 downsamplings \" \\\n",
    "                                                        \"with window_size %d in ms-ssim\" % ((window_size - 1) * (2 ** 4),window_size)\n",
    "\n",
    "    if weights is None:\n",
    "        weights = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]\n",
    "    weights = paddle.to_tensor(weights)\n",
    "\n",
    "    if window is None:\n",
    "        window = create_window(window_size, sigma, channel)\n",
    "    assert window.shape == [channel, 1, window_size, window_size], \" window.shape error\"\n",
    "\n",
    "    levels = weights.shape[0] # 5\n",
    "    mcs = []\n",
    "    for i in range(levels):\n",
    "        ssim_per_channel, cs =  _ssim(img1, img2, window=window, window_size=window_size,\n",
    "                                       channel=3, data_range=data_range,C=C, size_average=False)\n",
    "        if i < levels - 1:\n",
    "            mcs.append(F.relu(cs))\n",
    "            padding = [s % 2 for s in img1.shape[2:]]\n",
    "            img1 = avg_pool(img1, kernel_size=2, padding=padding)\n",
    "            img2 = avg_pool(img2, kernel_size=2, padding=padding)\n",
    "\n",
    "    ssim_per_channel = F.relu(ssim_per_channel)  # (batch, channel)\n",
    "    mcs_and_ssim = paddle.stack(mcs + [ssim_per_channel], axis=0)  # (level, batch, channel) 按照等级堆叠\n",
    "    ms_ssim_val = paddle.prod(mcs_and_ssim ** weights.reshape([-1, 1, 1]), axis=0) # level 相乘\n",
    "    #print(ms_ssim_val.shape)\n",
    "    if size_average:\n",
    "        return ms_ssim_val.mean()\n",
    "    else:\n",
    "        # 返回各个channel的值\n",
    "        return ms_ssim_val.flatten(2).mean(1)\n",
    "class MS_SSIMLoss(paddle.nn.Layer):\n",
    "   \"\"\"\n",
    "   1. 继承paddle.nn.Layer\n",
    "   \"\"\"\n",
    "   def __init__(self, loss_weight, reduction='mean', data_range=255., channel=3, window_size=11, sigma=1.5):\n",
    "       \"\"\"\n",
    "       2. 构造函数根据自己的实际算法需求和使用需求进行参数定义即可\n",
    "       \"\"\"\n",
    "       super(MS_SSIMLoss, self).__init__()\n",
    "       self.data_range = data_range\n",
    "       self.C = [0.01, 0.03]\n",
    "       self.window_size = window_size\n",
    "       self.channel = channel\n",
    "       self.sigma = sigma\n",
    "       self.window = create_window(self.window_size, self.sigma, self.channel)\n",
    "       self.loss_weight = loss_weight\n",
    "       # print(self.window_size,self.window.shape)\n",
    "   def forward(self, input, label):\n",
    "       \"\"\"\n",
    "       3. 实现forward函数，forward在调用时会传递两个参数：input和label\n",
    "           - input：单个或批次训练数据经过模型前向计算输出结果\n",
    "           - label：单个或批次训练数据对应的标签数据\n",
    "           接口返回值是一个Tensor，根据自定义的逻辑加和或计算均值后的损失\n",
    "       \"\"\"\n",
    "       # 使用Paddle中相关API自定义的计算逻辑\n",
    "       # output = xxxxx\n",
    "       # return output\n",
    "       return self.loss_weight * (1-ms_ssim(input, label, data_range=self.data_range,\n",
    "                      window = self.window, window_size=self.window_size, channel=self.channel,\n",
    "                      size_average=True,  sigma=self.sigma,\n",
    "                      weights=None, C=self.C))\n",
    "class CharbonnierLoss(nn.Layer):\n",
    "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
    "\n",
    "    def __init__(self, loss_weight=1.0, reduction='mean', eps=1e-3):\n",
    "        super(CharbonnierLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        diff = x - y\n",
    "        # loss = torch.sum(torch.sqrt(diff * diff + self.eps))\n",
    "        loss = paddle.mean(paddle.sqrt((diff * diff) + (self.eps*self.eps)))\n",
    "        return loss\n",
    "\n",
    "class CompositeLoss(nn.Layer):\n",
    "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
    "\n",
    "    def __init__(self, loss_weight=1.0, reduction='mean', eps=1e-3):\n",
    "        super(CompositeLoss, self).__init__()\n",
    "        self.SSIMLoss = MS_SSIMLoss(loss_weight=1.0)\n",
    "        self.L2Loss = CharbonnierLoss()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        loss = self.SSIMLoss(x, y) + self.L2Loss(x, y)\n",
    "        return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、测试细节\n",
    "由于测试图像大多为2K图像，直接预测图像会导致显存不足的问题，因此我们采用了裁剪预测的方式来进行，将每张输入图像在输入网络前平均裁剪成大小一致的4块，不能被2整除的图像进行padding再裁剪，最后通过网络预测出四个图像块，然后再通过拼接操作，拼接成输出图像来输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七、代码结构\n",
    "### code:\n",
    "- config: 配置文件\n",
    "- data: 定义数据增强函数\n",
    "- models: 定义网络模型以及损失函数\n",
    "- metrics: 定义评价指标函数\n",
    "- utils: 定义其他函数\n",
    "- dataset.py: 定义数据集\n",
    "- output: 模型训练输出文件夹\n",
    "- data_preparation.py: 训练数据裁剪脚本\n",
    "- data_preparation_all.py: 全量训练数据裁剪脚本\n",
    "- generate_meta_info.py: 数据筛选脚本\n",
    "- generate_meta_info_all.py: 全量数据筛选脚本\n",
    "- predict.py: 预测脚本\n",
    "- train.py: 训练脚本\n",
    "### test_code:\n",
    "- predict.py: 预测脚本\n",
    "- model.pdparams: 最佳权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 八、上分策略\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/2bffa4513a1f498da0945379160fa87e420dd753765d4604bc5071e76e07de34)\n",
    "\n",
    "\n",
    "上分策略主要集中在数据清洗、推理速度优化和损失函数优化:\n",
    "\n",
    "### 数据清洗\n",
    "可以看出测试数据和训练数据的domain不一致，因此采用下采样两倍后的训练数据可以极大提升网络性能。同时不必要的空白图块也会干扰网络学习到去模糊的知识，让网络误以为恢复空白图块可以恢复的很好导致收敛不充分。因此采用了数据筛选后，我们的模型性能可以进一步提升。\n",
    "\n",
    "### 推理速度\n",
    "\n",
    "原始代码推理速度1.67s,耗时严重，经分析，我们采用更小的模型，在精度损失不大的情况下可以获得0.83s的速度， 网络满足线上V100推理显存需求\n",
    "\n",
    "### 损失函数优化\n",
    "一个很明显的情况是线上PSNR的指标很高，因此需要考虑如何提升ms_ssim指标。在网络第一阶段收敛后，结合Charbonnier Loss和MS_SSIM Loss进行分数性能的提升。\n",
    "\n",
    "### Test-time Local Converter (TLC)优化\n",
    "由于训练是在一个固定的patch size上进行训练的，这导致在处理大图块的过程中，不能充分利用所有信息，因此采用了TLC的模型优化策略来进一步提升网络模型。同时也导致了一定的时间损耗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码启动过程\n",
    "## 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T01:16:31.851716Z",
     "iopub.status.busy": "2022-09-07T01:16:31.851238Z",
     "iopub.status.idle": "2022-09-07T01:18:25.142057Z",
     "shell.execute_reply": "2022-09-07T01:18:25.140650Z",
     "shell.execute_reply.started": "2022-09-07T01:16:31.851682Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.2 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "### 解压数据\n",
    "!cd data/ && unzip -q train_data_*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##创建训练数据目录\n",
    "! mkdir IMG\n",
    "! mkdir TAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##裁剪训练数据集\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "constprefix=\"train_data_\"\n",
    "rgb_dir='./'\n",
    "IMG='./IMG'\n",
    "TAR='./TAR'\n",
    "#####裁剪尺寸为1024x1024\n",
    "BASE=1024\n",
    "for i in range(1,6):\n",
    "    image_path=os.path.join(rgb_dir,constprefix + str(i))\n",
    "    annotation_path=os.path.join(image_path,\"annotation.txt\")\n",
    "    tar_filenames=[]\n",
    "    inp_filenames=[]\n",
    "    mask_filenames=[]\n",
    "    with open (annotation_path,\"r\") as f:\n",
    "            data=f.read().splitlines()\n",
    "            for i in range (len(data)):\n",
    "                cur=data[i].split(' ')\n",
    "                tar_filenames.append(os.path.join(image_path,cur[0]))\n",
    "                inp_filenames.append(os.path.join(image_path,cur[1]))\n",
    "                mask_filenames.append(os.path.join(image_path,cur[2]))\n",
    "    \n",
    "    for i in range(len(mask_filenames)):\n",
    "    \n",
    "        mask_path=mask_filenames[i]\n",
    "        inp_path=inp_filenames[i]\n",
    "        tar_path=tar_filenames[i]\n",
    "        filename=os.path.split(mask_path)[-1]\n",
    "        mask_img=cv2.imread(mask_path,0)\n",
    "        inp_img=cv2.imread(inp_path)\n",
    "        tar_img=cv2.imread(tar_path)\n",
    "        H,W=mask_img.shape\n",
    "        mask_img[np.where(mask_img>40)]=255\n",
    "        mask_img[np.where(mask_img<40)]=0\n",
    "        contours, hierarchy = cv2.findContours(mask_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        x, y, w, h = cv2.boundingRect(contours[0])\n",
    "        x1=x;x2=x+w;y1=y;y2=y+h;\n",
    "        w_add_half=BASE//2\n",
    "        h_add_half=BASE//2\n",
    "        x_mid=(x1+x2)//2\n",
    "        y_mid=(y1+y2)//2\n",
    "        if(x_mid-w_add_half<=0):\n",
    "            x1=0\n",
    "            x2=BASE\n",
    "        elif(x_mid+w_add_half>W):\n",
    "            x2=W\n",
    "            x1=W-BASE\n",
    "        else:\n",
    "            x1=x_mid-w_add_half\n",
    "            x2=x_mid+w_add_half\n",
    "        if(y_mid-h_add_half<=0):\n",
    "            y1=0\n",
    "            y2=BASE\n",
    "            \n",
    "        elif(y_mid+h_add_half>H):\n",
    "            y2=H\n",
    "            y1=H-BASE\n",
    "            \n",
    "        else:\n",
    "            y1=y_mid-h_add_half\n",
    "            y2=y_mid+h_add_half\n",
    "        inp_filename=os.path.split(inp_path)[-1]\n",
    "        tar_filename=inp_filename.split('.jpg')[0]+'_tar.jpg'\n",
    "        new_inp_path=os.path.join(IMG,inp_filename)\n",
    "        new_tar_path=os.path.join(TAR,tar_filename)\n",
    "        new_inp_img=inp_img[y1:y2,x1:x2,:]\n",
    "        new_tar_img=tar_img[y1:y2,x1:x2,:]\n",
    "        cv2.imwrite(new_inp_path,new_inp_img)\n",
    "        cv2.imwrite(new_tar_path,new_tar_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T01:18:50.714945Z",
     "iopub.status.busy": "2022-09-07T01:18:50.714306Z",
     "iopub.status.idle": "2022-09-07T01:18:52.110516Z",
     "shell.execute_reply": "2022-09-07T01:18:52.109476Z",
     "shell.execute_reply.started": "2022-09-07T01:18:50.714915Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.2 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## 去除不必要的zip文件\n",
    "! rm -rf *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.2 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## 安装训练所需的python包\n",
    "! pip install torch==1.7.1\n",
    "! pip install scikit-image\n",
    "! pip install warmup_scheduler\n",
    "! pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.2 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## 利用L1Loss训练\n",
    "! cd ../ && python train.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测过程\n",
    "按照官方给定的预测脚本运行方式相同，将最终的finetuning模型改名为model.pdparams，并且放在predict.py的同意目录下，使用命令：\n",
    "```\n",
    "python predict.py [src_image_dir] [results]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.2 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## 测试脚本\n",
    "! cd test_code/ && python predict.py {your_test_data_path} {save_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('torch17')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a95acce5b543d3c9f36313f15d009ac3a32893b3bb468982aa15fb18300d36f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
